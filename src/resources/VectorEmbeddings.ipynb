{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "# from classification import logisticRegression\n",
    "from collections import defaultdict\n",
    "# import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## Parse Yelp Reviews   ->   `([Reviews], [Labels])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yelp_parser\n",
    "\n",
    "def parse_yelp_reviews() -> ([str], [str]):\n",
    "    return yelp_parser.get_chi_hotel_review_score_list()\n",
    "\n",
    "reviews, scores = parse_yelp_reviews()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "## Convert each review into a `TaggedDocument`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_corpus(reviews: [str], scores: [int]):\n",
    "    stoplist = stopwords.words('english')\n",
    "    review_tokens = []\n",
    "    for review in reviews:\n",
    "        review_tokens.append([word for word in review.lower().split() if word not in stoplist])\n",
    "    for i, text in enumerate(review_tokens):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(text, [scores[i]])\n",
    "\n",
    "corpus = list(get_corpus(reviews, scores))[:20000]\n",
    "train_corpus, test_corpus = train_test_split(corpus, test_size=0.25, random_state=42)\n",
    "print(len(train_corpus))\n",
    "\n",
    "# from math import floor, ceil\n",
    "# train_corpus = all_data[0:floor(3*total_num_obs/4)]\n",
    "# test_corpus = all_data[floor(3*total_num_obs/4):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "## Train Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 50)\n"
     ]
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Doc2Vec\n",
    "\n",
    "model = Doc2Vec(window=100, dm=1, vector_size=50, min_count=2)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "train_targets, train_regressors = zip(*[(doc.words, doc.tags[0]) for doc in train_corpus])\n",
    "test_targets, test_regressors = zip(*[(doc.words, doc.tags[0]) for doc in test_corpus])\n",
    "\n",
    "X = []\n",
    "for i in range(len(train_targets)):\n",
    "    X.append(model.infer_vector(train_targets[i]))\n",
    "\n",
    "train_x = np.asarray(X)\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "## Add Unique Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9606666666666667\n"
     ]
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "Y = np.asarray(train_regressors)\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "labelEncoder.fit(Y)\n",
    "train_y = labelEncoder.transform(Y)\n",
    "print(np.mean(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9626\n"
     ]
    }
   ],
   "source": [
    "# def get_learning_vec(doc2vec_model, tagged_docs):\n",
    "#     targets, regressors = zip(*[(doc.tags[0], doc2vec_model.infer_vector(doc.words, steps=20)) for doc in tagged_docs])\n",
    "#     return targets, regressors\n",
    "    \n",
    "\n",
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression()\n",
    "logreg.fit(train_x, train_y)  \n",
    "\n",
    "test_list = []\n",
    "for i in range(len(test_targets)):\n",
    "    test_list.append(model.infer_vector(test_targets[i]))\n",
    "\n",
    "print(\"What is test_list::\" , test_list)\n",
    "test_x = np.asarray(test_list)\n",
    "test_Y = np.asarray(test_regressors)\n",
    "test_y = labelEncoder.transform(test_Y)\n",
    "\n",
    "predictions = logreg.predict(test_x)\n",
    "np.mean(test_y)\n",
    "\n",
    "acc = sum(predictions == test_y) / len(test_y)\n",
    "print(acc)\n",
    "\n",
    "# def logistic_regression(x, y, test_fraction=0.25):\n",
    "#     print(\"Logistic Regression\")\n",
    "#     print(\"-------------------\\n\")\n",
    "    \n",
    "#     print(\"Tagging Documents\")\n",
    "# #     x_train_tagged = list(get_corpus(x_train))\n",
    "# #     x_test_tagged  = list(get_corpus(x_test))\n",
    "    \n",
    "#     print(y_train)\n",
    "#     print(\"Creating Doc2Vec Model\")\n",
    "#     doc2vec_model = create_doc2vec_model()\n",
    "#     doc2vec_model.build_vocab(x_train_tagged)\n",
    "#     doc2vec_model.train(x_train_tagged, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs)\n",
    "    \n",
    "#     print(\"Building Learning Vectors\")\n",
    "#     y_vec_train, x_vec_train = get_learning_vec(doc2vec_model, x_train_tagged)\n",
    "#     y_vec_test,  x_vec_test  = get_learning_vec(doc2vec_model, x_test_tagged)\n",
    "    \n",
    "#     print(\"Creating Classifier\")\n",
    "#     classifier = linear_model.LogisticRegression(penalty='l2', fit_intercept=True)\n",
    "#     classifier.fit(x_vec_train, y_vec_train)\n",
    "    \n",
    "#     print(\"Training and Predicting\")\n",
    "#     functions.train_classifier_and_evaluate_accuracy_on_training_data(classifier, x_vec_train, y_vec_train)\n",
    "#     functions.train_classifier_and_evaluate_accuracy_on_testing_data(classifier, x_vec_test, y_vec_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
